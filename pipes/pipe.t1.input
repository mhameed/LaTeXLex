\documentclass{beamer}
\usepackage{beamerprosper}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgflibraryarrows}
\usepackage{pgflibraryshapes}
\usepackage{pgflibrarytikztrees}
\usepackage{pifont}
\usepackage{bibunits}

\def\vbar{\Pisymbol{pzd}{120}}

\usetheme[hideothersubsections]{Goettingen}
\useinnertheme{rectangles}

\def\sectionslide#1{\section{#1}}

\definecolor{keytext}{rgb}{0.35,0,0.65}%%{blue!65!red}
\definecolor{diagram}{rgb}{0.35,0,0.65}%%{blue!65!red}
\def\keytext#1{\textcolor{keytext}{#1}}

\title{Part I, Section 5\\
Non-standard interpretation}
\author{}
\institution{Non-standard interpretation --- rule of signs, strictness
analysis, staging transformations, partial evaluation, type checking
and type inference}
\begin{document}\maketitle
\DefaultTransition{Replace}

%------------------------------------------------------------------------

\begin{slide}{Non-standard interpretation}

\begin{itemize}

\item Also called ``absract interpretation'': invented by Cousot and
Cousot (1977).

\item Purpose: formalize various optimization techniques through a
common framework.

\item Why? Optimization often involves wanting answers to undecidable
questions.  Thus results are usually {\em yes\/} or {\em don't
know\/}, rather than {\em no\/}.

\item For example:

\begin{itemize}

\item is an expression constant for all executions of (parts of) this
program?
\item is an expression recalculated unnecessarily?
\item does the result of a function always depend on a particular
     parameter?
\item what parts of the program become constant if we fix the value of
     a particular parameter?
\end{itemize}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Rule of signs 1/2}

\begin{itemize}

\item Illustrate principle by deciding the sign of the result of a
product {\em without\/} computing the result:

\item Consider the operation $*$ over $\mathbb Z$.  The ``rule of
signs'' tells us the sign of the result using the domain $\mathbb
Z^{\#}$:

\[\mathbb{Z^{\#}} = {\mbox{plus}, \mbox{minus}, \mbox{zero}}\]

and a abstraction function from $\mathbb Z$ to $\mathbb Z^{\#}$:

\[abs(z) = \left\{\begin{array}{rl}
\mbox{plus} & \mbox{if}\; z>0\\
\mbox{minus} & \mbox{if\;} z<0\\
\mbox{zero} &  \mbox{if}\; z=0
	   \end{array}\right.
\]

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Rule of signs 2/2}

\begin{itemize}

\item Can now define an abstraction of multiplication to determine the
sign of the result of all expressions involving only products:

{\def\slidearraystretch{0.8}\scriptsize$\begin{array}{rcccl}
\mbox{plus} *^{\#} \mbox{plus}  &=& \mbox{minus} *^{\#} \mbox{minus} &=& \mbox{plus}\\
\mbox{plus} *^{\#} \mbox{minus} &=& \mbox{minus} *^{\#} \mbox{plus}  &=& \mbox{minus}\\
\mbox{zero} *^{\#} a     &=&     a *^{\#} \mbox{zero}  &=& \mbox{zero}
\end{array}$}

\item But with sums the sign cannot always be known:

{\def\slidearraystretch{0.8}\scriptsize$\begin{array}{rcccl}
 \mbox{plus} +^{\#} \mbox{plus}  &=& \mbox{zero} +^{\#} \mbox{plus}  &=&  \mbox{plus} +^{\#} \mbox{zero} = \mbox{plus}\\
\mbox{minus} +^{\#} \mbox{minus} &=& \mbox{zero} +^{\#} \mbox{minus} &=& \mbox{minus} +^{\#} \mbox{zero} = \mbox{minus}\\
 \mbox{zero} +^{\#} \mbox{zero}  &=& \mbox{zero}\\
BUT\\
\mbox{plus} +^{\#} \mbox{minus} &=& \mbox{minus} +^{\#} \mbox{plus} &=& ?
\end{array}$}

\item For completeness, state that:

{\def\slidearraystretch{0.8}\scriptsize$\begin{array}{rcccccl}
? *^{\#} a &=& a *^{\#} ? &=& ? *^{\#} ? &=& ?\\
? +^{\#} a &=& a +^{\#} ? &=& ? +^{\#} ? &=& ?
\end{array}$}

\item So, ``no information'' propagates

\end{itemize}

\end{slide}

%------------------------------------------------------------------------
\section{Strictness Analysis}
%------------------------------------------------------------------------

\begin{slide}{Strictness analysis: outline}

\begin{itemize}

\item Replace call-by-need with call-by-value when can prove it does
  not change the termination properties of the program.  Denote a
  non-terminating expression by $\bot$.

\item PhD thesis by Alan Mycroft (1981).  Informal idea is intuitive,
but correctness proof is complicated.

\begin{quote}
{\bf Definition:} a function is strict in its
i$^{th}$ argument if when that is $\bot$ the result of the
function is also $\bot$
\end{quote}

\item Program is interpreted such that the result says which
  parameters are {\em always} used and which are {\em sometimes} used,
  so the former can be passed by value.

\item The analysis is conservative, so some strict arguments may not
be detected, but no non-strict argument will be identified as strict.

\end{itemize}

\end{slide}

\begin{slide}{Domains for strictness}

\begin{itemize}

\item Define an abstract domain for $\mathbb A$, called $\mathbb A^{\#}$
  such that
\begin{eqnarray*}
\bot_{\mathbb A} & \rightarrow & 0_{{\mathbb A}^{\#}}\\
*_{\mathbb A} & \rightarrow & 1_{{\mathbb A}^{\#}}
\end{eqnarray*}
and $0<1$, ({\em false} and {\em true} resp.).

\item A {\em definitely non-terminating} expression is represented by
  $0$ and one that {\em may} (but may not) terminate by $1$, so the
  abstraction function is defined by:
\begin{eqnarray*}
abs_{\mathbb A}(\bot) &=& 0\\
abs_{\mathbb A}(a) &=& 1, (a \not= \bot, \in {\mathbb A})
\end{eqnarray*}

\item Hence, $f^{\#}(1,...,1,0,1,...,1) = 0$ means that $f$ is strict
in its $i^{th}$ argument.

\end{itemize}

\end{slide}

\begin{slide}{Semantics of -{}-C$^{\#}$}

\begin{itemize}

\item Need to define abstract versions of the primitive functions of
the language for the abstract domain

\item Binary arithmetic operators: strict in both arguments, so
  abstract versions are $\equiv$ to conjunction:
\begin{eqnarray*}
0 \wedge 0 & = & 0\\
0 \wedge 1 & = & 0\\
1 \wedge 0 & = & 0\\
1 \wedge 1 & = & 1
\end{eqnarray*}
\item Conditional expressions: $\mbox{{\em if}}^{\#}(p,q,r) = p \wedge
(q \vee r)$ because it depends on $p$ and either $q$ or $r$.

\item Functions: for simplicity, just consider first-order
  non-recursive functions.  Extension to recursive and high-order is
  fairly straightforward.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Example 1/2}

\begin{itemize}

\item Definition:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
fun f(x,y,z) = if (x=0) then (y+z) else (x-y);
\end{Verbatim}

\item Whose abstract representation is
\[f^{\#}(X,Y,Z) = (x=0)^{\#} \wedge ((y+z)^{\#} \vee (x-y)^{\#})\]

where $X$ represents $x^{\#}$ etc..  Have $(x=0)^{\#} = X \wedge 1$
since $=$ is binary strict and $0^{\#}$ is $1$ because it is a
constant.  Similarly for the other expressions, obtaining:
\begin{eqnarray*}
f^{\#}(X,Y,Z) &=& (X \wedge 1) \vee ((Y \wedge Z) \vee (X \wedge Y))\\
&=& X \wedge Y \wedge (X \vee Z)
\end{eqnarray*}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Example 2/2}

\begin{itemize}

\item We can now compute the strictness of $f$ by substituting $0$ for
  each argument in turn:
\begin{eqnarray*}
f^{\#}(0,1,1) &=& 0 \wedge 1 \wedge (0 \vee 1)\\
              &=& 0\\
f^{\#}(1,0,1) &=& 0\\
f^{\#}(1,1,0) &=& 1
\end{eqnarray*}

\item Conclusion: $f$ is strict in its first two parameters, but not
the third.

\end{itemize}

\end{slide}

%% %------------------------------------------------------------------------

%% \begin{slide}{Strictness: a  recursive example}

%% \begin{itemize}

%% \item The next problem in making this technique generally useful is to
%% be able to handle recursion abstractly.  The mechanism for recursive
%% functions is the same as for the non-recursive case, obtaining a
%% recursive abstract function, but because its domain is small (two
%% point!) we take advantage of it to calculate the least fixed point
%% directly:

%% \begin{Verbatim}[numbers=left,fontsize=\tiny]
%% fun f(x,y) = if (x=0) then y else f(x-1,y);
%% \end{Verbatim}

%% which abstracted is:

%% \begin{eqnarray}
%% f^{\#}(X,Y) & = & (x=0)^{\#} \wedge (y^{\#} \vee f^{\#}((x-1)^{\#}, y^{\#}))\\
%% & = & X \wedge (Y \vee f^{\#}(X,Y))
%% \end{eqnarray}

%% To find the least fixed point of $f^{\#}$ we use the following
%% iteration (general form without proof), where E is an expression
%% containing $f^{\#}$:

%% \begin{eqnarray}
%% f^{\#}(X_1,...,X_n) & = & E\\
%% f^{\#}_0(X_1,...,X_n) & = & 0\\
%% f^{\#}_n(X_1,...,X_n) & = & E[f^{\#}:=f^{\#}_(n-1)]
%% \end{eqnarray}

%% Applied to our function:

%% \begin{eqnarray}
%% f^{\#}_0(X,Y) = 0
%% f^{\#}_1(X,Y) = X \wedge (Y \vee 0) = X \wedge Y
%% f^{\#}_0(X,Y) = X \wedge (Y \vee (X \vee Y)) = X \wedge Y
%% \end{eqnarray}

%% so the least fixed point is $X \wedge Y$, telling us that $f$
%% is strict in both arguments.  Note that it is not in general (with n
%% arguments) sufficient to stop when consecutive iterations are the
%% same, rather the whole matrix must be computed and termination is when
%% the whole matrix is unchanged.  This is a highly simplified
%% description; the proofs of safeness involve some pretty heavy notation
%% and algebra.

%% \end{itemize}

%% \end{slide}

%------------------------------------------------------------------------
\section{Staging Transformations}
%------------------------------------------------------------------------

\begin{slide}{Staging transformations: 1/3}

\begin{itemize}

\item Staging is a general term for transformations that break a
computation into two parts (stages) so that the different stages may
be executed at different times.

\item Similar to but different from abstract interpretation

\item Familiar examples:

\begin{itemize}

\item Macros in C:
\begin{itemize}
\item use pre-processor to read the macro definitions and to interpret
the macro applications
\item compile the macro-free program
\end{itemize}

\item Macros in LISP: more complicated because LISP also provides
an interpreted environment as well as a compiler

\end{itemize}

\item {\bf Definition:} A staging transformation is a technique
for separating a process into two parts, so that given $p(x,y)$, it is
rewritten as two programs: $p_2(p_1(x),y)$.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Staging transformations: 2/3}

\begin{itemize}

\item A compiler can be viewed as the result of a staging transformation
on an interpreter: if the interpreter is $p$, then the compiler is $p_1$,
with the program $x$, then it is run by the run-time system $p_2$, with the
input $y$.  This is a major class of staging transformation called
partial evaluation.

\item What's the difference?  Consider this example:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
f(x,y) = (x+y)(x-y) (i.e. f(x,y) = x^2 - y^2)
\end{Verbatim}

\item staging {\tt f} w.r.t. {\tt x}, we obtain:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
f1(x) = x^2 and f2(f1(x),y) = f1(x) - y^2
\end{Verbatim}

\item But partially evaluating {\tt f} w.r.t. a value for {\tt x},
gives:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
fx(y) = x^2 - y^2, for some fixed value of x
\end{Verbatim}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Staging transformations: 3/3}

\begin{itemize}

\item Another example:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
f(x,y) = cos(x+y) (i.e. f(x,y) = cos x * cos y - sin x * sin y)
\end{Verbatim}

\item And partial evaluation yields a similar cosmetic result to that
above:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
fx (y) = ax * cos y - bx sin y, where ax = cos x and bx = sin x
\end{Verbatim}

\item But with staging w.r.t. {\tt x}, we obtain:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
f1(x) = (cos x,sin x)
\end{Verbatim}

\item and 

\begin{Verbatim}[numbers=left,fontsize=\tiny]
f2(f1(x),y) = f1(x).first * cos y + f2(x).second * sin y
\end{Verbatim}

\item Staging first identified in 1978 by J{\o}rring and Scherlis; mostly
applied to the construction of language implementations, but is more
general.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------
\section{Partial Evaluation}
%------------------------------------------------------------------------

\begin{slide}{Partial evaluation: 1/2}

\begin{itemize}

\item An automatic tool for program optimization, like but stronger
than an optimizing compiler.

\item It is a source-to-source staging transformation: a program {\tt
p} and partial data {\tt s} are transformed into {\tt p-s} by
precomputing parts of {\tt p} that depend only on {\tt s}.

\item The possibility, in principle, of partial evaluation is
contained in Kleene's classical s-m-n theorem.

\item Valuable when {\tt p} runs for a long time, and {\tt p-s}
is significantly faster than {\tt p}.

\item Example applications: ray tracing, FFT, circuit and planetary
simulations. Partial evaluators have also been used to compile using
interpreters for programming languages and to generate compilers from
interpreters.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Partial evaluation: 2/2}

\begin{itemize}

\item Partial evaluation is the specialisation of a program
w.r.t. some of its arguments.  Thus:
\[P_1(x_1,...,x_n) \rightarrow P_2(x_2,...,x_n)\]


\item Called restriction in analysis and currying in logic.

\item Difference is that the domain and co-domains are not functions
but program texts.  Contrast with constant propagation in compilers:
but more general and more complicated.

\item Danger is that the process can fail to terminate and, of course,
this cannot be predicted in advance.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Power example: 1/2}

\begin{itemize}

\item The primary objective of PE is to improve efficiency by
replacing $P_1$ by a simpler program $P_2$, in which $x_1$ is fixed.
Consider the case of computing {\tt x} raised to the power {\tt n}:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
int power(int n, int x) {
  return (n==1 ? x : x*power(n-1,x);
}
\end{Verbatim}

\item Specialize power wrt {\tt n=3}, and unfold the recursion:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
power(3,x) = x*power(2,x)
           = x*x*power(1,x)
           = x*x*x
\end{Verbatim}

\item or, can define successive specialized functions:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
power3(x) = x*power2(x)
power2(x) = x*power1(x)
power1(x) = x
\end{Verbatim}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Power example: 2/2}

\begin{itemize}

\item But specialize wrt {\tt x=5} and unfold; the process does not
terminate:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
power(n,5)
 = (n==1 ? 5 : 5*power(n-1,5))
 = (n==1 ? 5 : 5*(n==1 ? 5 : 5*power(n-1-1,5))
 = ...
\end{Verbatim}

\item Want a different kind of solution, in which {\tt x} gets
fixed, and create a more specialized recursive function:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
power(n,5)
 = int power5(int n) { return (n==1 ? 5 : 5*power5(n-1)); }
\end{Verbatim}

\item The problem is how to work out what to unfold.  Another
non-standard interpretation, called Binding Time Analysis (BTA)
provides (most of) the answers.

\item PE is particularly relevant to programming language
implementation.  Originally only good for lambda calculus, but now C
is possible.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{The first Futamura projection}

\begin{itemize}

\item Program P is applied to IN1 and IN2, giving OUT:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
OUT = P(IN1,IN2)
\end{Verbatim}

\item Assume there is a partial evaluator called MIX:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
PIN1 = MIX(P,IN1)
 OUT = PIN1(IN2)
     = (MIX(P,IN1))(IN2)
\end{Verbatim}

\item Now assume we have an interpreter INT:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
OUT = SOURCE(IN1,...,INN)
    = INT(SOURCE,IN1,...,INN)
    = (MIX(INT,SOURCE))(IN1,...,INN)
    = TARGET(IN1,...,INN)
\end{Verbatim}

\item That is, the interpreter is specialized to run program P so it
only contains the operations for P, ie. P has been compiled: {\tt
target = mix(int,source)}.  This is the first Futamura projection.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{The second Futamura projection}

\begin{itemize}

\item Now introduce the concept of compilation and that MIX can be
applied to itself:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
  TARGET = MIX(INT,SOURCE)
         = (MIX(MIX,INT))(SOURCE)
COMPILER = MIX(MIX,INT)
\end{Verbatim}

\item That is, the partial evaluator is specialized by the
interpreter, giving rise to a compiler.  This is the second Futamura
projection.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{The third Futamura projection}

\begin{itemize}

\item Can now construct a compiler generator by an additional
application of MIX:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
OUT   = P(IN1,IN2)
      = (MIX(P,IN1))(IN2)
      = ((MIX(MIX,P))(IN1))(IN2)
      = (((MIX(MIX,MIX))(P))(IN1))(IN2)
COGEN = MIX(MIX,MIX)
\end{Verbatim}

\item That is, an interpreter can be transformed automatically into a
compiler.  This is the third Futamura projection.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------
\sectionslide{Type Checking and Type Inference}
%------------------------------------------------------------------------

\begin{slide}{Type Systems}

\begin{itemize}

\item Types allow some correctness aspects of programs to be verified

\item Types act as constraints:

\begin{itemize}

\item No types: no checking, all errors at execution

\item Monomorphic types: simple checking, increased programmer load

\item Polymorphic types: worst-case high complexity, reduced
  programmer load

\end{itemize}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Polymorphism}

\begin{itemize}

\item Polymorphism permits typing of

\begin{itemize}

\item Functions that are independent of the data type they process

\item Data structures that are independent of the data type of their
  elements

\end{itemize}

\item List length illustrates both aspects:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
int length(l) {
  if empty(l) then return 0;
  return 1+length(tail(l));
}
\end{Verbatim}

\item Parametric vs. ad-hoc polymorphism: for examples see above.
  Implementation is completely different.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Checking vs. Inference}

\begin{itemize}

\item Checking verifies that value usage is consistent with type
  declarations---like assertions

\begin{itemize}

\item Monomorphic or lexical: type names are unique and types are
  compared by name

\item Polymorphic: type schemes involving type variables permitting
  parameterized types.

\end{itemize}

\item Inference computes the type declarations that are consistent
  with value usage.

\begin{itemize}

\item Polymorphic {\em inference} type systems are strictly less
  powerful than polymorphic {\em checking} type systems (see Appel
  p.369).  Also known as {\em implicit} and {\em explicit} resp.

\end{itemize}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Type inference}

\begin{itemize}

\item A natural implementation of an implicitly typed language is to
  interpret the source program and construct a new AST decorated with
  explicit types

\item The decorated list length function might be:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
int length(l:list<a>) {
  if empty<a>(l) then return 0;
  return 1+length<a>(tail<a>(l));
}
\end{Verbatim}

\item Where {\tt list<a>} is a type constructor parameterized by a
  type variable.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{A type language}

\begin{itemize}

\item In addition to the expression language, there is also a type
  language used to write type expressions:

\begin{itemize}

\item {\bf type constants:} {\tt int}, {\tt string}, {\tt char}...

\item {\bf type constructors:} pairing/tupling---Cartesian product,
  e.g. {\tt int * int}, arrows/functions, e.g. {\tt int}
  $\rightarrow$ {\tt string} and {\tt list} e.g. {\tt int * list<a>
  $\rightarrow$ a}

\item {\bf type variables:} denoting polymorphism, e.g. $a \rightarrow
  b$ or in full $\forall a,b: a \rightarrow b$, known as shallow
  parametric polymorphism

\item The interpretation of this language is the basis for the
  computation of the type of the program.

\end{itemize}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Polymorphic type checking}

\begin{itemize}

\item Define an interpreter over the type expressions associated with
  programming language terms to check constraints

\begin{itemize}

\item {\bf variable declaration:} extend environment associating name
  with type

\item {\bf block/let:} defines scope of type bindings

\item {\bf function definition:} build new type environment
  initialized with bindings of formal parameters, (check body and
  result type)

\item {\bf function application:} substitute actual parameters for
  formals in signature and check consistency.

\end{itemize}

%% \item Type variables (in substitutions) essentially act like scopes
%%   for types

\item Use unification (over the type language) to establish
  equivalence.  See Appel algorithm 16.8, p.360.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Polymorphic type inference}

\begin{itemize}

\item Process is analogous to the checking process, building a type
  environment and interpreting the type language.

\item Difference is that in checking the value of a type is stated,
  but in inference it is initially unknown.

\begin{Verbatim}[commandchars=\\\{\},
codes={\catcode`$=3\catcode`^=7},
numbers=left,fontsize=\tiny]
$\alpha$ length(l:$\beta$) \mbox{{\tt{}\{}}
  if empty(l) then return 0;
  return 1+length(tail(l));
\mbox{{\tt \}}}
\end{Verbatim}

\item But through the constraints supplied by other parts of the
  program, such as {\tt 1+} and {\tt tail} can establish that
  $\alpha$={\tt int} and $\beta$={\tt list<a>}.

\item Hence obtaining:

\begin{Verbatim}[numbers=left,fontsize=\tiny]
int length(l:list<a>) {
  if empty<a>(l) then return 0;
  return 1+length<a>(tail<a>(l));
}
\end{Verbatim}

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{The type inference algorithm}

\begin{itemize}

\item The type language is extended with type {\em metavariables\/}:
  placeholders for actual types

\item See algorithm 16.10, p.365

\item Unification process extended to handle metavariables

\item Inferred type may then contain metavariables: this is when
  polytypes are created, by {\em generalization}:
\[\mbox{{\bf Meta}}(\alpha) \rightarrow \mbox{{\bf Meta}}(\alpha)\]
becomes
\[\forall \alpha: \alpha \rightarrow \alpha\]

\item Subsequently, when an inferred polytype is {\em specialized} (or
  {\em instantiated}), the bound type variables are replaced with
  metavariables ({\em not} variables).
\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Generalization example 1/2}

\begin{itemize}

\item Blocks/variables are the key to polymorphism in the
Hindley-Milner algorithm.  Consider:

\begin{Verbatim}[numbers=left,fontsize=\tiny,baselinestretch=0.8]
(function f(g) { return(pair(g(1),g("abc"))); })
  (function i(x) { return x; })
\end{Verbatim}

fails because:

\begin{itemize}

\item g first gets the type $t_1$

\item the first application updates this to $\mbox{int}\rightarrow
  t_2$,

\item but at the second application, unification of
  $\mbox{int}\rightarrow t_2$ and $\mbox{string}\rightarrow t_3$
  fails.

\end{itemize}

\item Note: this could be type-{\em{}checked\/} because the programmer
  may assert the correct polymorphic type.

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Generalization example 2/2}

\begin{itemize}

\item Alternatively:

\begin{Verbatim}[numbers=left,fontsize=\tiny,baselinestretch=0.8]
{ function g = function i(x) { return x; };
  return pair(g(1),g("abc"));
}
\end{Verbatim}

\item The block/variable binding rule types {\tt i} first and it gets
the type $\forall t_1 : t_1\rightarrow t_1$ (a generic type):

\item All the free variables in a generic type are universally
quantified (shallow typing).

\item When a generic type is used it is specialized (instantiated) by
replacing the bound variables from the context of use---int and string
in this case.

\item Thus different applications of the same function in the same
  scope may have different types

\end{itemize}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Directed Reading}

\begin{bibunit}[unsrt]
Ch.16 of Appel is of major relevance and although it is not that easy
reading, it is probably the most approachable presentation of
Hindley-Milner currently available.

Jones \cite{jones:1996} provides a readable description of partial
evaluation.

{\tiny\renewcommand{\refname}{References}
\putbib[cm30171]
}
\end{bibunit}

\end{slide}

%------------------------------------------------------------------------

\begin{slide}{Summary, section 5}

\begin{itemize}

\item Key idea: can interpret a program to find out properties instead
of the answer

\item Typically such problems risk non-termination

\item Compilation is just one well-behaved, well-defined,
well-understood case

\item Other examples are: type inference, strictness analysis,
variable lifetime analysis (regions), binding time analysis (for PE)

\item Recognize that interpretation is not just a function from
programs to answers (standard semantics), but from programs to results
that can be programs, types or whatever we want to find out
(non-standard semantics).

\end{itemize}

\end{slide}

\end{document}

%------------------------------------------------------------------------

\begin{slide}

\end{slide}

